# -*- coding: utf-8 -*-
"""Copia de Ejercicio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13U76fbodXMcDe3wldwqmwVBo4sWenhdj

# Librerías a emplear
"""

import matplotlib.pyplot as plt  
import numpy as np  
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.cm as cm

"""# **1 . Grafica y obten el valor mínimo/máximo de la siguientes funciones** ([help](https://colab.research.google.com/drive/1o57iUVzKWifaQ3W3tjmmrw33yk8RlGfp?usp=sharing)):

  **1.1** f(x,y) = x^2+y^6+y^5-y^3

  **1.2** f(x,y) =-50+x^2+y^2

  **1.3** f(x,y) =-abs(x)-y^2

------------------------------------------------
*Solución al problema*

Funciones a utilizar para la solución del problema.
"""

#FUNCIÓN PARA REALIZAR LAS GRÁFICAS

def graficar(Z,X,Y):
  # Gráficar la superficie
  fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
  grf = ax.plot_surface(X, Y, Z, cmap=cm.cool,
                        linewidth=0, antialiased=False)
  fig.colorbar(grf)
  return grf

#Función para derivar la función
def derivate(_p,p):
    return  (f(_p[0],_p[1]) - f(p[0],p[1])) / h

"""--------------------------------------------------------------------------------
**1.1 f(x,y) = x^2+y^6+y^5-y^3**



"""

def f(x,y):
    return (x**2 + y**6+ y**5-y**3)

puntos=100  #Puntos a graficar
X=np.linspace(-4, 4,puntos)  #Puntos a graficar de X en el rango -10, 10
Y = np.linspace(-4, 4, puntos)  #Puntos a graficar de Y en el rango -10, 10
X, Y = np.meshgrid(X, Y)                #Malla en 3D
Z=f(X,Y)  #Función resultant

graficar(Z,X,Y)

level_map = np.linspace(np.min(Z), np.max(Z),puntos) 
plt.contourf(X, Y, Z, levels=level_map,cmap=cm.cool)
plt.colorbar()
plt.title('Descenso del gradiente')

# función:                 x**2 + y**6+ y**5-y**3
# Definición de derivada: f(x+h) - f(x) / h, siendo h: (x+h)-x

def derivate(_p,p):
    return  (f(_p[0],_p[1]) - f(p[0],p[1])) / h

p = np.random.rand(2)*8-4 # generar dos valores aleatorios

plt.plot(p[0],p[1],'o', c='k')

lr = 0.01
h = 0.001

grad = np.zeros(2)

for i in range(100):
    
    # Enumerate nos enumera los componente del punto.
    for idx, val in enumerate(p): 
        _p = np.copy(p)
        _p[idx] = _p[idx] + h; 
        dp = derivate(_p,p) 
        grad[idx] = dp
        
    p = p - lr * grad
    if(i % 10 == 0): plt.plot(p[0],p[1],'o', c='r')

plt.plot(p[0],p[1],'o', c='w')
plt.show()

print("El punto mínimo/máximo se encuentra en: ", p)

"""**1.2 f(x,y) =-50+x^2+y^2** """

def f(x,y):
    return (-50 + x**2+ y**2)

puntos=100  #Puntos a graficar
X=np.linspace(-4, 4,puntos)  #Puntos a graficar de X en el rango -10, 10
Y = np.linspace(-4, 4, puntos)  #Puntos a graficar de Y en el rango -10, 10
X, Y = np.meshgrid(X, Y)                #Malla en 3D
Z=f(X,Y)  #Función resultant

graficar(Z,X,Y)

level_map = np.linspace(np.min(Z), np.max(Z),puntos) 
plt.contourf(X, Y, Z, levels=level_map,cmap=cm.cool)
plt.colorbar()
plt.title('Descenso del gradiente')

# función:                 x**2 + y**6+ y**5-y**3
# Definición de derivada: f(x+h) - f(x) / h, siendo h: (x+h)-x

def derivate(_p,p):
    return  (f(_p[0],_p[1]) - f(p[0],p[1])) / h

p = np.random.rand(2)*8-4 # generar dos valores aleatorios

plt.plot(p[0],p[1],'o', c='k')

lr = 0.01
h = 0.001

grad = np.zeros(2)

for i in range(100):
    
    # Enumerate nos enumera los componente del punto.
    for idx, val in enumerate(p): 
        _p = np.copy(p)
        _p[idx] = _p[idx] + h; 
        dp = derivate(_p,p) 
        grad[idx] = dp
        
    p = p - lr * grad
    if(i % 10 == 0): plt.plot(p[0],p[1],'o', c='r')

plt.plot(p[0],p[1],'o', c='w')
plt.show()

print("El punto mínimo/máximo se encuentra en: ", p)

"""**1.3 f(x,y) =-abs(x)-y^2**"""

def f(x,y):
    return (np.abs(x)-y**2)

puntos=100  #Puntos a graficar
X=np.linspace(-4, 4,puntos)  #Puntos a graficar de X en el rango -10, 10
Y = np.linspace(-4, 4, puntos)  #Puntos a graficar de Y en el rango -10, 10
X, Y = np.meshgrid(X, Y)                #Malla en 3D
Z=f(X,Y)  #Función resultant

graficar(Z,X,Y)

level_map = np.linspace(np.min(Z), np.max(Z),puntos) 
plt.contourf(X, Y, Z, levels=level_map,cmap=cm.cool)
plt.colorbar()
plt.title('Descenso del gradiente')

# función:                 x**2 + y**6+ y**5-y**3
# Definición de derivada: f(x+h) - f(x) / h, siendo h: (x+h)-x

def derivate(_p,p):
    return  (f(_p[0],_p[1]) - f(p[0],p[1])) / h

p = np.random.rand(2)*8-4 # generar dos valores aleatorios

plt.plot(p[0],p[1],'o', c='k')

lr = 0.01
h = 0.001

grad = np.zeros(2)

for i in range(100):
    
    # Enumerate nos enumera los componente del punto.
    for idx, val in enumerate(p): 
        _p = np.copy(p)
        _p[idx] = _p[idx] + h; 
        dp = derivate(_p,p) 
        grad[idx] = dp
        
    p = p - lr * grad
    if(i % 10 == 0): plt.plot(p[0],p[1],'o', c='r')

plt.plot(p[0],p[1],'o', c='w')
plt.show()

print("El punto mínimo/máximo se encuentra en: ", p)

"""# **2. Respondan a las siguientes preguntas:**

**2.1 ¿Que sucede si incrementas la tasa de aprendizaje?**

La tasa de aprendizaje hace referencia a la cantidad de cambios/ iteraciones que se les da al punto de inicio, por lo que al incrementar su valor se disminuye el número de las iteraciones.


Es decir si tenemos una tasa de 0.1 el cambio de la derivada será de 0.1 en 0.1. Mientras que si tenemos un valor alto, por ejemplo de 2 el cambio de la derivada será de 2 en 2, y serán menos los puntos en los cuales se itera.

La tasa de aprendizaje debe de elegirse en función al gradiente.

**2.2 ¿Siempre resulta el mismo punto mínimo/máximo?** 
Dependiendo del valor aleatorio en el cual se inician las iteraciones variará el valor minimo/máximo encotrado, tambien depende de la tasa de aprendizaje utilizada.

# Repaso

Escribe un tensor de 4 dimensiones usando numpy, es decir, que al usar tensor.shape de 4 valores (v1,v2,v3,v4)
"""

import tensorflow as tf

tensor_4= np.array([[[[1,2,3],]]])
tensor_4

tf.shape(tensor_4)